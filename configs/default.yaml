environment:
  success_easy_habitual: 0.7
  success_hard_habitual: 0.4
  success_easy_deliberate: 0.9
  success_hard_deliberate: 0.7
  habitual_effort_cost: 0.15
  deliberate_effort_cost: 0.4
  reward_easy_success: 1.0
  reward_hard_success: 2.0
  failure_penalty: -0.6
  habitual_fatigue_increment: 0.02
  deliberate_fatigue_increment: 0.08
  fatigue_failure_penalty: 0.08
  fatigue_success_relief: 0.10
  fatigue_recovery: 0.08
  fatigue_hard_task_penalty: 0.05
  observation_noise: 0.04
  difficulty_flip_chance: 0.3
  rest_recovery: 0.20
  rest_reward_penalty: -0.1
  passive_recovery_rate: 0.02
  deliberate_fatigue_penalty: 0.6
  habitual_fatigue_penalty: 0.2

agent:
  epsilon: 0.1
  alpha: 0.25
  initial_efficiency: 0.9
  fatigue_penalty: 0.45
  habitual_hard_penalty: 0.3
  deliberate_easy_penalty: 0.1

simulation:
  horizon: 200
  num_episodes: 50
  log_dir: results/logs
  plot_dir: results/plots
  record_trajectories: true
